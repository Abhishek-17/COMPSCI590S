\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{graphicx}
\usepackage{url}

%
% The following commands sets up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\dnl}{\mbox{}\par}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
  \newpage
  \setcounter{lecnum}{#1}
  \setcounter{page}{1}
  \noindent
  \begin{center}
  \framebox{
     \vbox{\vspace{2mm}
   \hbox to 6.28in { {\bf COMPSCI~590S~~~Systems for Data Science
                       \hfill Fall 2016} }
      \vspace{4mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
      \vspace{2mm}
      \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe(s): #4} }
     \vspace{2mm}}
  }
  \end{center}
  \markboth{Lecture {#1}: #2}{Lecture {#1}: #2}
  \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
%
\renewcommand{\cite}[1]{[#1]}

% \input{epsf}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{FIGURE-SIZE}{CAPTION}{FILENAME}
\newcommand{\fig}[4]{
           \vspace{0.2 in}
           \setlength{\epsfxsize}{#2}
           \centerline{\epsfbox{#4}}
           \begin{center}
           Figure \thelecnum.#1:~#3
           \end{center}
   }

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% Some useful equation alignment commands, borrowed from TeX
\makeatletter
\def\eqalign#1{\,\vcenter{\openup\jot\m@th
 \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
     \crcr#1\crcr}}\,}
\def\eqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\llap{$##$}\tabskip\z@skip\crcr
   #1\crcr}}
\def\leqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\kern-\displaywidth\rlap{$##$}\tabskip\displaywidth\crcr
   #1\crcr}}
\makeatother

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:



% Some general latex examples and examples making use of the
% macros follow.

\begin{document}

%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{25}{Dynamo and Basics of ML [Tensor Flow] }{Emery Berger}{Namrita Pandita, Mark Saad}

\section{Virtual Nodes [Used in Dynamo] }
Why use Virtual Nodes?
\begin{itemize}
\item	For handling heterogeneity of Nodes. 
\begin{itemize}
\item	A Node with 1TB Memory can handle more load than the one having just 256MB. Placing more Virtual Nodes for the machine with higher capacity can proportionally handle the load. 
\end{itemize}
\item	For load balancing.
\begin{itemize}
\item	Virtual Nodes ensure that if a node goes down, its load gets proportionally divided among other active nodes. 
\end{itemize}
\end{itemize}

\section{Gossip Protocol [Used in Dynamo] }
Broadcast are easy but it scales horribly, gossip protocol is peer based (peer to peer) and scales infinitely.
\par
\par
Reason for Scalabilty:
\begin{itemize}
\item Gossip Protocol uses P2P. i.e. No matter how big the cluster is, one node communicates with just one other node at a time.
\item	How does it work?
\begin{itemize}
\item	It randomly chooses a peer instead of a fixed peer and they trade messages and update state
\item Repeats the process every so often
\item	No deterministic guarantees, entirely probabilistic
\item Rapid Dissemination of messages
\item Eventually converges. i.e. All nodes will eventually receive all the messages.
\end{itemize}
\end{itemize}
\section{Does Dynamo implement ACID ?}
\begin{itemize}
\item	It doesn’t. The only guarantee is that the key-value operations are atomic
\end{itemize}
\section{Basics of Machine Learning}
\begin{itemize}
\item	Two types of ML:
\begin{itemize}
\item	Supervised (has a label)
\item 	Unsupervised (no label)
\end{itemize}
\item ML is used mainly for classification (sometimes binary classification)
\item Cross validation: 
\begin{itemize}
\item	you train ML algorithm on a subset of the data and then see how it classifies the rest of the data. 
\item You then calculate the accuracy of that classification (precision and recall: which is a fancy way of saying false positives and false negatives)
\end{itemize}
\item The goal of ML is to come up with a loss function that converges. The loss function is able to find the wrong predictions (both false positives and false negatives)
\item Feature Engineering:
\begin{itemize}
\item	ML started with the idea of feature engineering (where you train the machine to classify based on certain features you choose, much like an “if this then that” tree) 
\item  Feature engineering proved to be hard and ineffective and that’s why we transitioned to neural networks
\end{itemize}
\item Neural Network
\begin{itemize}
\item	For neural nets to work you need a lot of data, you may also want to call it BIG DATA
\item  Big training sets are slow to converge and compute intensive.
\item TPUs are specific chips[ASIC] that Google designed and built to use to run the tensor flow classifications in production
\end{itemize}
\end{itemize}

\end{document}